\chapter{Limitations}
\label{chap:limitations}

We now discuss limitations of the systems presented in this dissertation. We
start with limitations that are common to all three systems, and then discuss
limitations of each individual system.

\section{Limitations common to all three systems}

\Para{Lack of silicon implementations.} One major implementation limitation is
that none of our three systems has a hardware implementation in silicon. This
is a problem that crops us whenever a new system that needs hardware
modifications needs to be evaluated. Designing a new chip can take a few years,
and it requires both a significant financial investment and access to large
teams of engineers.

 Instead, we evaluated these systems using a combination of simulation and
microbenchmarks. We briefly considered an FPGA implementation on the NetFPGA
platform~\cite{netfpga}, but decided against it because the relative areas
consumed by router subsystems on an FPGA may not accurately reflect the relative
area consumptions on a router ASIC.\footnote{Even though an FPGA implementation
will not give us the right area assessments, there may still be value to an
FPGA implementation, as a vehicle for evaluating ideas end-to-end, much like we
did with our Mininet evaluation (\S\ref{sec:eval:mininet}).}
%TODO: There is a contradiction between the previous paragraph and the next one.

That said, we paid careful attention to keeping the hardware designs simple,
clearly specified the interfaces between the different hardware blocks, and
evaluated the area consumed by our hardware designs by synthesizing them to a
recent transistor library. The development of new FPGA technology that allows
clock rates of up to 1 GHz on some FPGA designs~\cite{hyperflex} suggests a way
to prototype hardware at near-ASIC speeds while benefiting from the cheaper
non-recurrent-enginerring cost and flexibility of an FPGA.

\Para{Lack of completeness theorems.} Another major limitation with our systems
is that we do not have a definitive formal characterization of what can and
cannot be done by each system. While we have examples of algorithms that cannot
be expressed by each system, it would be ideal to have a ``completeness''
theorem that states that all algorithms within a class (and within that class
alone) can be expressed using a particular system. Such a theorem would also
give a formal assurance of future proofness so long as a new algorithm fell
within that class. An example of such a theorem is the equivalence of
finite-state machines and regular expressions, which states that any regular
expression---whether a regular expression we know of today or a regular
expression that comes up in the future---can be expressed in terms of a
finite-state machine.

\Para{Lack of a user study.} In the absence of a completeness theorem that
guarantees a level of future proofness, one way to empirically study the
``future proofness'' of a programming abstraction is to freeze the abstraction
and then let new programmers who have never seen the abstraction before program
it. We have not been able to carry out a formal user study using network
programmers because the concept of programming a network is itself relatively
new and has not reached mainstream adoption yet!

At the same time, we have had some encouraging results that suggest that some
of our abstractions and primitives may be future proof. For instance, as
mentioned earlier, the atoms developed in Domino (Chapter~\ref{chap:domino})
turned out to be useful for the performance queries that were not
linear-in-state (Chapter~\ref{chap:perf_query}). Similarly, we did not
anticipate the rate-controlled service disciplines (RCSD) as a use case for
PIFOs when first designing PIFOs, but later found that PIFOs could express the
RCSD class. Finally, the fact that the atomic semantics of  packet
transactionss could be adopted as such into P4 suggests that the packet
transaction semantics might be quite general and natural. That said, conducting
a user study to understand the practical limitations of expressiveness is an
interesting area for future work.


%Two major limitations of this work: is it future proof and nothing is implemented
%in a sense, the two are related, maybe implementing things would have forced us
%to be more future proof, because there is no longer the temptation of modifying things
% once you have taped out an ASIC (you just can't!!)

\section{Domino}
\label{sec:domino_limitations}
The \pktlanguage compiler doesn't aggressively optimize, instead focusing on
generating sub-optimal, but correct pipeline configurations. For instance, it is
possible to fuse two stateful codelets incrementing two independent counters
into the same instance of the Pairs atom. However, by carrying out a one-to-one
mapping from codelets to the atoms implementing them, our compiler precludes
these optimizations.  Developing an {\em optimizing} compiler for packet
transactions is an area for future work.

Supporting multiple packet transactions in \pktlanguage also requires further
work, especially because any real program running on a router is likely to
execute multiple transaction, each on a subset of the packets seen by the
router. When a router executes multiple transactions, there may be
opportunities for inter-procedural analysis~\cite{dragonbook}, which goes
beyond compiling individual transactions and looks at multiple transactions
together.  For instance, the compiler could detect computations common to
multiple transactions and execute them only once.

Finally, we have a manual and ad hoc design process for atoms. Currently, we
use a process of trial and error to first guess atoms and then use our compiler
to check if those atoms can support useful algorithms.  Formalizing this design
process and automating it into an atom-design tool would be useful when
designing router instruction sets. For instance, given a corpus of data-plane
algorithms, this tool would automatically mine the corpus for recurring motifs
of state and packet header modification. A router hardware engineer could then
design hardware for atoms that capture these motifs.

\section{PIFOs}
\label{sec:pifo_limitations}

Our programming model for scheduling based on PIFOs is a scheduling tree with
scheduling and shaping transactions. While the scheduling tree makes it
\textit{possible} to program new scheduling algorithms using PIFOs, it does not
make it \textit{easy}. We have found that it requires considerable effort to
program new scheduling algorithms using scheduling trees.  In other words,
scheduling trees are still a low-level abstraction. We need to raise the level
of abstraction if we hope to make programmable scheduling more broadly
accessible to network operators.

Beyond a few counter examples, we lack a formal characterization of the
scheduling algorithms that cannot be implemented using PIFOs. For instance, is
there a simple, checkable property separating algorithms that can and cannot be
implemented using PIFOs? Given an algorithm specification, can we automatically
check if the algorithm can be programmed using PIFOs?

Our current PIFO design scales to 2048 flows. If these 2048 flows are allocated
evenly across 64 ports in a 64-port 10G router, we could program scheduling
across 32 flows at each port. This permits per-port scheduling across traffic
aggregates (\eg fair queueing across 32 VMs/tenants within a server), but not
a finer granularity (\eg 5-tuples). Ideally, to schedule at the finest
granularity, our flow scheduler would realize the ideal PIFO where each packet
in the PIFO belongs to a different flow. We currently support only 2048 flows,
while we can support up to 60K packets.  More design work is required to close
this gap between the number of flows and the number of packets and realize an
idealized PIFO.

%TODO: Do something about the stuff at the top of pifo_discussion.tex. It's not used anywhere right now.

\section{Performance queries}
\label{sec:pq_limitations}

The major limitation of performance queries is that the latest value of the
aggregated state is not available in the data plane, and is only accessible to
software reading the backing store. This manifests itself most when using the
{\ct emit()} statement in our programs, which requires access to the latest value of the
aggregated state in the data plane. Currently, we handle such queries by not
evicting those entries from a key-value store if the state stored in those
entries is emitted to another query downstream. But not evicting keys severely
limits the scalability of the key-value store because all keys must fit within
the small on-chip cache. Unfortunately, this is unavoidable if we want access to the latest
value of state during {\ct emit()}s.

A similar scalability limitation affects queries that are not linear-in-state
because there is no way to evict key-value pairs while still guaranteeing
correctness if the query is not linear-in-state. In practice, we have found
(\S\ref{sec:workaround-nonscalable}) that it is possible to rewrite measurement
functionality in such a way that the resulting query is linear-in-state.
