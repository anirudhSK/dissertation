\chapter{Introduction}
\label{chap:intro}

\section{Background}

%TODO: Diagram of net. arch.
Computer networks have two classes of elements: the \textit{end hosts} that
generate packets and the \textit{routers}\footnote{We use the term router to
refer to both switches and routers in this disseration.} that forward these
packets between the end hosts. Historically, the Internet was architected so
that most of the complexity resided in the end hosts, while the routers
themselves were simple. According to Clark~\cite{design_philosophy}, this
architecture was a result of the overarching design goal of the Internet: the
ability to easily interconnect existing networks with disparate network
architectures (\eg long-haul networks, local-area networks, satellite networks,
and radio networks) while providing acceptable end-to-end connectivity. Quoting
Clark, "The Internet architecture achieves this flexibility by making a minimum
set of assumptions about the function which the net will provide."

The minimum functionality assumed of and provided by the network was
best-effort and unreliable packet forwarding. Notably absent from a router's
feature set were reliable packet delivery, packet prioritization, monitoring
features to attribute a router's resource usage to specific end hosts, and
security features to detect network breaches. As a result, the early routers
were singularly dedicated to packet forwarding. A minimal router feature set
made it simpler to design high-speed routers and helped broaden the Internet's
reach by interconnecting existing networks with minimum friction. But, it
sidelined other goals~\cite{design_philosophy} such as improving network
performance, security, and monitoring.
 
Today, four decades after ideas underlying the Internet were first
published~\cite{cerf74}, it is clear that routers need to do much more than
forward packets for at least two reasons. First, once the basic goal of
interconnecting different networks is achieved, other goals like performance,
security, and monitoring rise in prominence.  Second, many large-scale private
networks (\eg datacenters, private wide-area networks, enterprise networks) do
not need to concern themselves with interconnecting disparate networks as the
Internet had to and hence can expect more from their network. As a result, a
typical router today implements many features beyond packet forwarding,
pertaining to security (\eg access control), monitoring (\eg counting the
number of packets belonging to each flow transiting the router), and
performance (\eg priority queues).

However, despite the feature creep in routers, there's little consensus between
network operators and router vendors on a router's feature set. Inevitably,
there are network operators whose needs fall outside their router's feature
set. But because today's fastest routers are built out of specialized
forwarding hardware, they are largely {\em fixed-function}\footnote{The term fixed-function
was first used to describe graphics processing units (GPUs) with limited or no programmability~\cite{gpu_fixed}. We use it in an analogous sense here.} in that their
functionality cannot be changed once the router has been built. In such cases,
the operator has no alternative but to wait two--three years for the next
generation of the router hardware---best illustrated by the lag time between
the standardization and availability of new overlay protocols~\cite{vxlan,
nvgre}.

As a result, the rate of innovation in new router algorithms is outstripping
our ability to get these algorithms into production routers, especially those
routers that run at speeds in excess of a Tbit/s.
Figure~\ref{fig:router_algos} shows a timeline of prominent router algorithms
that have been developed since the 1980s. Of these, only a handful are
available in production routers because there is no way to program a new router
algorithm on a production router.
%TODO: What is a production router?

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{router_alg_timeline.pdf}
\caption{Timeline of prominent router algorithms since the 1980s. Only the ones
shaded in blue are available on production routers today.}
\label{fig:router_algos}
\end{figure}

As an operator who wants to introduce new functionality in their network, what
are the operator's choices? One is to give up on changing routers altogether
and make all the required changes at end hosts as the original Internet did.
However, relying solely on end hosts results in solutions that are cumbersome
or suboptimal. As a first example, imagine measuring the queuing latency at a
particular hop in the network. One could do this by collecting end-to-end ping
measurements between a variety of vantage points and then fusing these
measurements together to estimate per-hop queueing latency. Not only is this
indirect, it is also inaccurate relatively to directly instrumenting the router
at that hop to measure its own queueing latency. As a second example, consider
the problem of congestion control, which divides up a network's capacity fairly
among competing users. There are many in-network solutions to congestion
control~\cite{xcp, rcp}, which outperform the end-host-only approaches to
congestion control used today~\cite{cubic, compound}. But, there is no way to
deploy them. 

Another alternative is to use a \textit{software router}: a catch-all term for
a router built on top of some programmable substrate, such as a general-purpose
CPU~\cite{click, routebricks}, a network processor\footnote{A CPU with an
instruction set tailored to packet processing~\cite{ixp4xx, ixp2800}.}, a
graphics processing unit (GPU)~\cite{packetshader}, or a field-programmable gate array (FPGA)~\cite{netfpga}.
Figure~\ref{fig:router_evolution} tracks the evolution of aggregate capacity of
software routers and compares them to the fastest routers known at that point
in time. The figure shows two trends. First, up until the mid 90s, software
routers were in fact the fastest routers; the early routers~\cite{imp} were
minicomputers loaded with forwarding software. Second, since the mid 90s,
growing demands for higher link speeds, fueled by the Internet's growth, have
meant that the fastest routers are now built out of dedicated hardware,
specialized for packet forwarding.

Hardware specialization gives these routers a 10--100 $\times$ performance
improvement relative to software routers.  This performance improvement is the
result of fully exploiting the abundant parallelism available in packet
processing. First, data parallelism, the ability to simultaneously process
either different parts of the same packet or packets belonging to different
ports. Second, pipeline parallelism, the ability to simultaneously perform
different operations on different packets. But, hardware specialization carries
a cost: because routers are built out of specialized hardware, they are
fixed-function devices that can not be reconfigured in the field.

Recent work in software-defined networking~\cite{openflow} (SDN) and
programmable switching chips~\cite{rmt, xpliant, flexpipe} has endowed fast
routers with limited flexibility. SDN allows operators to program the network
control plane, which is the part of the network that computes a network's
routing tables, by moving route computations out of the routers and on to a
programmable server. Programmable switching chips allow operators to program
parts of the data plane, which is the part of the network that forwards packets
based on the routing tables, such as packet header manipulations that do not
modify router state.  However, these solutions are still not sufficient to
express the grayed-out algorithms shown in Figure~\ref{fig:router_algos}
because (1) these algorithms programmatically manipulate router state and (2)
they require flexibility in packet scheduling, which is untouched by both SDN
and programmable switching chips.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{router_evolution.pdf}
\caption{Aggregate capacity of routers since the first router on the ARPANET in
1969~\cite{imp}. Until the mid 90s, software routers were sufficient. Since
then, however, the fastest routers have been built out of dedicated hardware.}
\label{fig:router_evolution}
\end{figure}

\section{Primary contributions}
\input{contributions_table}

This dissertation considers the problem of building routers that approach the
speeds of today's fastest fixed-function routers, while also being
programmable. My thesis is that {\em it is possible to design router hardware
that is both fast and programmable, if we restrict ourselves to programming
specific classes of router functionality}. It is this specificity that allows
us to resolve the programmability-performance tension; indeed, our designs
provide a much more restricted form of programmability than a Turing-complete
processor.  The challenge here is to pick classes of router functionality that
are simultaneously (1) practically useful to network operators, (2) broad
enough to cover a range of current and future use cases within that class, and
yet (3) narrow enough to permit a high-speed hardware implementation. We will
describe high-speed programmable hardware primitives and their corresponding
programming models in software for three classes of router functionality:
stateful data-plane algorithms, packet scheduling, and scalable network
measurement.  Table~\ref{tab:contributions} summarizes our contributions.

%Consider adding a crisp statement of limitations of each class of algorithms
%Packet payload processing, Algorithms with loop bounds only known at run time \\

\subsection{Stateful data-plane algorithms}
First (Chapter~\ref{chap:domino}), we consider the problem of programming {\em
stateful data-plane algorithms} at high packet processing rates. These are
algorithms that operate on a sequeunce of packets in a streaming manner, doing
a bounded amount of work per packet and manipulating a bounded amount of router
state in the process.  They include algorithms for managing the router's buffer
(\eg RED~\cite{red}, BLUE~\cite{blue}), load balancing (\eg CONGA~\cite{conga},
Flare~\cite{flare}), and in-network congestion control (\eg XCP~\cite{xcp},
RCP~\cite{rcp}).

High-speed data-plane programming poses two challenges: (1) what hardware
instructions are required to support programmable state modification at the
router's line rate and (2) what is the right programming model? To address
these challenges, we develop a system for data-plane programming, Domino, which
contains three main components, described below.

\Para{Atoms.} \textit{Atoms} capture a router's instruction set. They specify atomic
units of packet processing provided by the router hardware, \eg an atomic
counter or an atomic test-and-set. Atoms are atomic in the sense that if some
state is updated by an atom as part of processing a packet, the next packet
arriving at that atom will see the updated value of that state. The processing
within atoms is constrained to meet the atomicity requirement by ensuring that
the input-to-output latency of the atom's digital circuit is under a clock
cycle, so that the output is updated to the correct value in time for the next
packet arriving at the atom a clock cycle later. Figure~\ref{fig:simple_atom}
shows an example atom.

\Para{Packet Transactions.} \textit{Packet transactions} provide a programming model for data-plane
algorithms. A packet transaction is an atomic and isolated block of code
capturing an algorithm's logic written in a domain-specific language (DSL)
called Domino. Figure~\ref{fig:simple_transaction} shows an example packet transaction.
Packet transactions provide programmers with the illusion that the transaction's body
executes serially from start to finish on each packet, with
no overlap in packet processing across packets---akin to an infinitely fast single-threaded
processor carrying out packet processing on each packet. Packet
transactions are expressive and capture many important data-plane algorithms.
Further, their serial semantics shield programmers from the router's data and pipeline
parallelism. Packet transactions have been adopted in P4~\cite{p4}, a
packet-processing language that is emerging as an industry standard for
programming router chips.  P4 programmers can now use an @atomic annotation
around a block of statements to specify that the block must execute atomically.

\Para{The compiler.} The Domino compiler compiles packet transactions written
in the Domino DSL to a pipeline of atoms provided by the router, and rejects
the code if the router's atoms cannot support the packet transaction.

The compiler has three phases. First (\S\ref{ss:preprocessing}), it
preprocesses the code to make it easier to infer dependencies between
packet-processing operations. Second (\S\ref{ss:pipelining}), the compiler then
transforms the preprocessed, but still serial, packet transaction into a
parallel pipeline of {\em codelets}. When pipelining, the compiler maintains
the invariant that if each codelet executes atomically and passes off its
results to the next codelet, the behavior will be indistinguishable from the
packet transaction itself executing atomically on each packet. Third
(\S\ref{ss:code_gen}), the compiler maps each codelet one-to-one to an atom
provided by the underlying router, rejecting the code if the codelet cannot be
supported by the atom.

The compiler provides an {\em all-or-nothing} guarantee: if the compiler
compiles the packet transaction it will run at the line rate of the router; all
other programs that cannot run at line rate---because there isn't an atom to
carry out the program's operations at line rate---will be rejected. A
conventional compiler for a general-purpose CPU compiles all programs, but a
program's run-time performance depends on its complexity. In Domino, only
programs that are simple enough to run at the router's line rate will be
compiled, obviating the need for any performance profiling.
%TODO: Figure for compiler

\Para{Evaluation.}
Developing atoms for a router is a chicken-and-egg problem. A router's atoms
determine what algorithms the router can support, while the algorithms
determine what atoms are required in the first place. Designing the right atoms
is especially important for a programmable line-rate router because---unlike a
general-purpose CPU---there is no way to ``emulate'' functionality in software
when hardware support is not available.

\begin{figure}[!t]
\begin{minipage}{0.48\textwidth}
\centering
\vspace{0.38in}
\includegraphics[width=0.85\textwidth]{atom.pdf}
\caption{An atom that either adds either a constant or a packet field to a
piece of state x and writes it back to x.}
\label{fig:simple_atom}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.75\textwidth]{packet_transaction.pdf}
\caption{A packet transaction that samples the src IP address of every 10th
packet. State variables (count) are in red.}
\label{fig:simple_transaction}
\end{minipage}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.4\columnwidth]{iterative_design_process.pdf}
\caption{Iterative atom design process}
\label{fig:iterative_design}
\end{figure}

To develop atoms, we use the Domino compiler to experiment with different atoms
and iteratively modify the atoms until they support enough algorithms
(Figure~\ref{fig:iterative_design}).  Using this process, we developed seven
atoms of increasing complexity (Table~\ref{tab:templates}) that allow us to
progressively program more and more algorithms from
Figure~\ref{fig:router_algos}. For instance, measurement using Bloom filters
only requires simple read and write operations on state. On the other hand,
heavy-hitter detection~\cite{opensketch} uses a counting
sketch~\cite{count_min} that relies on a atomic counter. Finally, algorithms
like flowlet switching~\cite{flare} require conditional updates to counters.

In two subsequent projects~\cite{hula, perf_query} that we carried out after
the Domino work, we found that we could reuse the same atoms for other use
cases, providing evidence that these atoms generalize and are useful beyond the
algorithms that influenced their design in the first place.

\subsection{Programmable packet scheduling}
Packet scheduling is an important determinant of network performance. The
choice of scheduling algorithm is tied to a network's overarching goals. For
instance, an algorithm that divides link capacity fairly is ideal in a
multi-tenant setting~\cite{wfq}, while the shortest remaining processing time
algorithm is ideal for a single tenant who desires low flow completion
time~\cite{pFabric}. Today's routers provide a fixed set of scheduling
algorithms. While configuration settings on these scheduling algorithms can be
tweaked, there is no way for an operator to program a new scheduling algorithm
that is tailored to their needs.

Routers lack programmable scheduling because there is no single abstraction to
express many scheduling algorithms~\cite{wfq, srpt, srr, pFabric, lstf, csz}
that appear so different at first brush. This lack of a single unifying
abstraction is not merely an academic concern. It has practical consequences:
in the absence of a single abstraction that can then be hardened in hardware,
we are left with using a general-purpose substrate such as a CPU to program
schedule. As we mentioned earlier, this can seriously hurt performance.

 Push In First Out Queues (PIFOs) (Chapter~\ref{chap:pifo}) provide such an
abstraction.\footnote{PIFOs were first used as a proof construct to establish
the equivalence of combined input-output queued routers and purely output
queued routers~\cite{pifo}. Our contribution is to show that they can gainfully
be used for programmable packet scheduling.} They exploit our observation that
in many practical schedulers, the relative order of packets that are already
buffered does not change in response to new packet arrivals
(\S\ref{s:deconstruct}). Hence, when a packet arrives, it can be pushed into
the right location based on a packet priority (push in), but packets can always
be dequeued from the head (first out). A PIFO is simply a
priority queue of packets with a small program to assign each packet its
priority. Yet, by flexibly programming a packet's priority assignment, a
network operator can use the PIFO abstraction to program a variety of
previously proposed scheduling algorithms. So far, these algorithms could only
be run in simulation or on software routers.

\Para{Programming model for packet scheduling.} Our programming model for
scheduling couples a PIFO with a program to determine a packet's {\em rank} in
the PIFO. This rank can denote the packet's scheduling order (for
work-conserving algorithms such as WFQ) or absolute wall-clock departure time
(for non-work-conserving algorithms such as traffic shaping). This program is
written as a packet transaction, introduced earlier.  Depending on whether the
program determines the scheduling order or time, the program is called either a
scheduling transaction or a shaping transaction.

%TODO: Figure of WFQ as a scheduling transaction
%TODO: Figure of traffic shaping as a shaping transaction
A single PIFO coupled with a scheduling or shaping transaction can express many
classical scheduling algorithms, \eg token bucket shaping
(Figure~\ref{fig:tbf_trans}), and weighted fair queueing
(Figure~\ref{fig:wfq_trans}). But, a single PIFO is still restricted to
scheduling algorithms with the property that the relative order of packets that
are already buffered does not change in response to new packet arrivals. A
canonical class of scheduling algorithms that violate this relative order
property is the class of hierarchical scheduling algorithms~\cite{hpfq}.
 
To support hierarchical scheduling, we extend our programming model from a
single PIFO to a tree of PIFOs, while allowing an entry in a PIFO to be either
a packet or a reference to another PIFO. More formally, a node in a {\em scheduling
tree} (Figure~\ref{fig:scheduling_tree}) has three attributes: (1) a predicate
that determines which packets are handled by that node, (2) a scheduling
transaction that determines a packet's rank in a scheduling PIFO attached to
that node, and (3) an optional shaping transaction that determines a packet's
rank in an optional shaping PIFO.

We now explain the semantics of a scheduling tree during dequeues and enqueues.
During a dequeue, we walk the scheduling tree starting from the root. We
dequeue the scheduling PIFO at the root, resulting in either a reference to
another scheduling PIFO or a packet. If the result is a packet, we are done,
and we transmit the packet. If not, we continue recursively, dequeueing from
the scheduling PIFO that is pointed to until we find a packet.

When a packet is enqueued, we walk the tree from the leaf whose predicate
captures the packet to the root, executing scheduling and (optionally) shaping
transactions along the way. The scheduling transaction inserts either a packet
or a PIFO reference into the scheduling PIFO. The shaping transaction at a node
inserts a reference to the node's scheduling PIFO in the node's shaping PIFO.
When that reference is dequeued from the shaping PIFO, it is enqueued into the
node's parent's scheduling PIFO. It'll eventually be dequeued from the parent's
scheduling PIFO when it makes its way to the head of the parent's scheduling
PIFO. The shaping PIFO thus provides an optional mechanism to {\em defer}
enqueues into a parent's scheduling PIFO. Figure~\ref{fig:shaping_timing}
illustrates the timing of various operations related to a scheduling and a
shaping PIFO.

%TODO: Push-In First-Out or push-in first-out?
%TODO: Should we capitalize every word in Weighted Fair Queueing or not? Just be consistent.
% This may help: https://english.stackexchange.com/questions/40427/capitalization-of-explanation-of-abbreviations
The PIFO-based programming model unifies several scheduling algorithms and
allows us to program a wide variety of scheduling algorithms using the single
construct of the push-in first-out queue, \eg Weighted Fair
Queueing~\cite{wfq}, Token Bucket Filtering~\cite{tbf}, Hierarchical Packet
Fair Queueing~\cite{hpfq}, Least-Slack Time-First~\cite{lstf}, the
Rate-Controlled Service Disciplines~\cite{rcsd}, and fine-grained priority
scheduling (\eg Shortest Job First).

\Para{Hardware for PIFOs.} Supporting this programming model is a high-speed
hardware design for PIFOs. When designing hardware for PIFOs, we set out to
meet performance targets that are typical for a single-chip shared-memory
router today. These are routers that are built out of a single
application-specific integrated circuit (ASIC), and whose packet buffer and
scheduling logic is shared across all ports. Sharing the packet buffer and
scheduling logic reduces the memory and digital logic cost associated with
packet scheduling.

For concreteness, we picked a target clock frequency of 1 GHz to reflect a
requirement of performing one enqueue and one dequeue every nanosecond (typical
of high-speed routers today~\cite{rmt}). We targeted a packet buffer of size 12
MByte based on the buffer size of the Broadcom Trident
II~\cite{trident2_buffer}, a recent commercial single-chip shared memory
router. With a cell size of 200 bytes,\footnote{A cell is the minimum unit of
memory allocation in the packet buffer.} a 12 MByte buffer can support up to
60000 packets.

A naive way to implement a 60K-entry PIFO is to use a flat sorted array of 60K
elements and insert an incoming element into this array in a manner reminiscent
of insertion sort. Concretely, an incoming element's rank would be compared in
parallel to the ranks of all 60K elements producing a bitmask denoting which
elements were greater than or lesser than the incoming element. Because the
PIFO is sorted, the bitmask would have a single 0-to-1 transition, the position
of which could be detected using a priority encoder. Finally, the new element
could be inserted into this position. The problem with this approach is that it
is hard to lay out 60K parallel comparators, one for each element in the PIFO.

Instead, we exploit the observation that in most practical scheduling
algorithms, the scheduling is really across flows and not packets, because
packet ranks increase monotonically within a flow to prevent packet reordering
within a flow. Because ranks increase monotonically within a flow, we only need
to look at the first packet of each flow to determine which packet to dequeue
next. This reduces the number of elements that need to be sorted from 60K
packets in the naive implementation to around 1K flows (again, we picked this
number with reference to the Broadcom Trident II that supports \textasciitilde10 queues
on each of its \textasciitilde100 ports).

We find that transistor technology has evolved to the point where it is
relatively cheap to build a sorted array of 1K flows, where a flow can be
enqueued into or dequeued from the array every nanosecond. In a recent
industry-standard 16 nm technology node, a hardware design for a programmable
5-level hierarchical scheduler costs less than 4\% additional chip area
relative to a 200 \milli\meter\squared baseline router
chip~\cite{glen_parsing}.

\subsection{Scalable network measurement}

%% TODO: Functional query language.
%% TODO: Formally characterize linear-in-state operations here.
%% TODO: Maybe give an example of why merging is hard in general and why there's
%% something non-trivial here.

\TheSystem (Chapter~\ref{chap:perf_query}) provides a query language and
supporting router hardware for network performance measurement queries. As
examples, an operator could ask for TCP flows with a high degree of packet
reordering or a moving average of packet latencies for each flow. Our query
language allows order-dependent aggregation of information across packets
belonging to a single flow (\eg an exponentially weighted moving average over
packet latencies), while traditional query languages (\eg SQL) only support
order-independent aggregates like counts and averages. We designed a
programmable key-value store that runs in the router's ASIC to support these
aggregations.  The keys represent flows, while the values store and
programmatically update per-flow state (\eg counters or a moving average
filter) on every packet.

%TODO: Diagram of the Marple's k-v store and contrast to standard k-v store
At the heart of our key-value store is a technique to aggregate information
across packets belonging to a flow, while scaling to a large number of flows.
In order to scale to a large number of flows, we use a split design for our
key-value store with an on-chip cache within the router's ASIC supported by a
backing store sitting outside the router. A traditional cache would fetch the
correct value from the backing store on a miss and incur variable access
latencies in the process. Instead, our cache treats a cache miss as a packet
from a new flow. When this flow is eventually evicted from the cache, it is
merged with the current value for this flow in the backing store.  We
mathematically characterize the set of aggregation functions that permit such
merging, and show that it includes many useful aggregation functions such as
rolling minimums, rolling maximums, counters, predicated counters, statistics
computed over a bounded window of packets, and moving average filters.

\section{Towards a world of programmable networks}

The results in this dissertation point towards a world of programmable
networks, where network operators \textit{tell} routers what to do, without
being \textit{told} that this is all that the router can do. In such a setting,
operators could customize networks as they see fit.  They could program
additional features that give them performance benefits.  More interestingly,
they could remove needless features from their router, allowing them to
simplify their routers' feature sets, which in turn could ease troubleshooting
when things go wrong.

Besides the benefits to network operators, a programmable router chip has
benefits for router vendors as well. Router vendors can now add new features in
firmware and sell different versions of the firmware to different market
segments. Further, when bugs arise, it is much easier to fix these bugs in
firmware, as opposed to redoing the hardware design for the router, which could
easily take years. It also allows vendors to respond to new requests from
network operators in a period of days as opposed to years.

Whether the classes of router programmability described here will be sufficient
and future proof remains to be seen, but we are encouraged by the fact that the
hardware designs proposed here support a wide range of existing use cases and
several new use cases that we had not anticipated initially. We hope these
results provide guidance to router chip manufacturers when designing hardware
for programmable routers.

%%Lessons for programmable routers
%% HW/SW co-design.

%% Think vertically and design the entire stack from the ground up.
%% Results in simpler solutions because you have more degrees of freedom.
%% While there are undeniable benefits to working with commodity hardware components because
%% of ready deployment, there is also substantial performance and performance clarity to
%% be gained from vertically integrated designs because you open up the black box

%% Judicious use of hardware and exposing it the right way in software is a big win.

%% Non-Turing-complete designs have considerable richness to them. Turing complete processors
%% have ruled the roost for many years but the end of Moore's law suggests that that might
%% end very soon. This thesis can be seen as applying that kind of design philosophy to one
%% specific domain: high-speed packet forwarding.
