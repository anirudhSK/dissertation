%\section{Introduction}
\label{s:intro}

The previous chapter described the development of Remy, our automated
protocol-design tool. Remy generates congestion-control schemes from
first principles, given an uncertain model of the underlying network
and a stated application objective to pursue.

In practice, the designer of a congestion-control scheme for the
wide-area Internet cannot expect to supply a protocol-design tool with
a perfectly faithful model of the actual Internet. An actual model
will necessarily be simplified and imperfect. Nonetheless, designers
would naturally like to develop protocols that are broadly useful on
real networks.

In this chapter, we use Remy to quantify how much tolerance the
protocol designer can expect, or in other words: \emph{How easy is it
  to ``learn'' a network protocol to achieve desired goals, given a
  necessarily imperfect model of the networks where it will ultimately
  be deployed?}

Under this umbrella, we examine a series of questions about what
knowledge about the network is important when designing a
congestion-control protocol and what simplifications are acceptable:

\begin{enumerate}

\item \textbf{Knowledge of network parameters.} Is there a tradeoff
  between the performance of a protocol and the breadth of its
  intended operating range of network parameters~\cite{wroclawski}?
  Will a ``one size fits all'' protocol designed for networks spanning
  a thousand-fold range of link rates necessarily perform worse than
  one targeted at a subset of networks whose parameters can be more
  precisely defined in advance?  (\S\ref{s:oprangeperftradeoff})

\item \textbf{Structural knowledge.} How faithfully do protocol
  designers need to understand the topology of the network? What are
  the consequences of designing protocols for a simplified network
  path with a single bottleneck, then running them on networks with
  more-complicated structure?  (\S\ref{ss:topological})

\item \textbf{Knowledge about other endpoints.} In many settings, a
  newly-designed protocol will need to coexist with traffic from
  incumbent protocols. What are the consequences of designing a
  protocol with the knowledge that some endpoints may be running
  incumbent protocols whose traffic needs to be treated fairly
  (e.g.~the new protocol needs to divide a contended link evenly with
  TCP), versus a clean-slate design?  (\S\ref{s:tcpaware})

\item \textbf{Different kinds of breadth.} Will a protocol designed
  for a large range in one kind of network parameter (e.g., the link
  rate) also have broad applicability when other network parameters
  vary, such as the RTT or degree of multiplexing?  Which pairs of
  network parameters are related in this way---such that a protocol's
  breadth in one implies breadth in the other---and which aren't?
  (\S\ref{ss:parameter})

\item \textbf{Knowledge of network signals.} What kinds of congestion
  signals are important for an endpoint to listen to? How much
  information can be extracted from different kinds of feedback, and
  how valuable are different congestion signals toward the protocol's
  ultimate goals?  (\S\ref{s:signals})

\end{enumerate}

%\subsection*{RemyCCs as proxies for the best known algorithm}

Each of the above areas of inquiry is about the effect of a protocol
designer's imperfect understanding of the future network that a
decentralized congestion-control protocol will ultimately be run
over. In principle, we could quantify such an effect by evaluating, on
the actual network, the performance of the ``best possible''
congestion-control protocol designed for the imperfect network
\emph{model}, and comparing that with the best-possible protocol for
the actual network itself.

In practice, however, there is no tractable way to calculate the
optimal congestion-control protocol for a given network. Instead,
throughout this chapter we use Remy to construct achievability
results---RemyCCs---that stand in for the ``best possible'' average
performance across the range of network parameters supplied by the
designer.

We emphasize that there can be no assurance that Remy's algorithms
actually comes close to the optimal congestion-control protocols,
except to the extent that they approache analytical upper bounds on
performance and outperform existing schemes across the same networks.

\section{Summary of results}

The principal findings of these experiments are as follows:

\vspace{\baselineskip}

\noindent 
\textbf{Modeling a two-bottleneck network as a single bottleneck hurt
  performance only mildly.}  On the two-hop network of
Figure~\ref{fig:two-link}, on average we find that a protocol designed
for a simplified, single-bottleneck model of the network
underperformed by {\bf 17\%} a protocol that was designed with full
knowledge of the network's two-bottleneck structure
(Figure~\ref{f:multihop}). The simplified protocol outperformed TCP
Cubic-over-sfqCoDel by $2.75\times$ on average. In this example, full
knowledge of the network topology during the design process was not
crucial.

\vspace{\baselineskip}

\noindent 
\textbf{Only weak evidence of a tradeoff between
  operating range and performance.} We created a RemyCC designed
for a range of networks whose link rates spanned a thousand-fold range
between 1~Mbps and 1000~Mbps, as well as three other protocols that
were more narrowly-targeted at hundred-fold, ten-fold, and two-fold
ranges of link rate (Figure~\ref{fig:breadth}).

The ``thousand-fold'' RemyCC achieved close to the peak performance of
the ``two-fold'' RemyCC. Between link rates of 22--44~Mbps, the
``thousand-fold'' RemyCC achieved \textbf{within 3\% of the throughput}
of the ``two-fold'' protocol that was designed for this exact
range. However, the queueing delay of the ``thousand-fold'' protocol
was \textbf{46\% higher}, suggesting some benefit from more focused
operating conditions. It also takes Remy longer to optimize (offline)
a ``thousand-fold'' RemyCC compared with a two-fold RemyCC. In run-time
operation, the computational cost of the two algorithms is
similar.

Over the full range of 1~Mbps to 1000~Mbps, the ``thousand-fold''
RemyCC protocol matched or outperformed TCP Cubic and
Cubic-over-sfqCoDel over the entire range
(Figure~\ref{fig:breadth}). This result suggests that it may be
possible to construct ``one size fits all'' end-to-end
congestion-control protocols that perform considerably better than
TCP, even when TCP is assisted by in-network algorithms.

\vspace{\baselineskip}

\noindent \textbf{TCP-awareness hurt performance when TCP
  cross-traffic was absent, but helped dramatically when present.} We
measured the costs and benefits of ``TCP-awareness''---designing a
protocol with the explicit knowledge that it may be competing against
other endpoints running TCP, compared with a ``TCP-naive'' protocol
for a network where the other endpoints only run the same TCP-naive
protocol.

When contending only with other endpoints of the same kind, the
``TCP-naive'' protocol achieved \textbf{55\% less queueing delay} than
the TCP-aware protocol. In other words, ``TCP-awareness'' has a cost
measured in lost performance when TCP cross-traffic is absent
(Figure~\ref{fig:tcpaware}).

But when contending against TCP, the ``TCP-naive'' protocol was
squeezed out by the more-aggressive cross-traffic. By contrast, the
``TCP-aware'' protocol achieved \textbf{36\% more throughput} and
\textbf{37\% less queueing delay} than the ``TCP-naive'' protocol,
and reached an equitable division of the available capacity with TCP.
