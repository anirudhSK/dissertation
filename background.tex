\chapter{Background and Related Work}
\label{chap:related}

%TODO: Check dates on all subsections
As background for the rest of this dissertation, we first review several prior
approaches to programmability in networks (\S\ref{s:bgnd_bgnd}). We then review
work that is closely related to and concurrent with the work presented in this
dissertation (\S\ref{s:concurrent}).  This chapter focuses on literature that
is generally related to the broad themes of this dissertation.
Chapters~\ref{chap:domino}, \ref{chap:pifo}, and \ref{chap:perf_query} discuss
literature that is more specifically related to each of our three systems.

\section{A history of programmable networks}
\label{s:bgnd_bgnd}

\subsection{Minicomputer-based routers (1969 to the mid 1990s)}
The first router on a packet-switched network was likely the Interface Message
Processor (IMP) on the ARPANET in 1969~\cite{imp}. The IMP described in the
original IMP paper~\cite{imp} was implemented on the Honeywell DDP-516
minicomputer. In today's terminology, such a router would be called a software
router because it was implemented as software on top of a general-purpose
computer.

This approach of implementing routers on top of minicomputers was sufficient
for the modest forwarding rates required at the time. For instance, the IMP
paper reports that the IMP's maximum throughput was around 700 kbit/s,
sufficient to service several 50 kbit/s lines in both directions.  Such
minicomputer-based routers were also eminently programmable: changing the
functionality of the router simply required upgrading the forwarding software
and loading the minicomputer with a new piece of software.

This approach of building production routers using minicomputers continued into
the mid 1990s.  A notable example of a software router during the 1970s was David
Mills' Fuzzball router~\cite{fuzzball}. The most well known examples from the
1980s were Noel Chiappa's C Gateway~\cite{cgw}, which was the basis for the MIT
startup Proteon~\cite{proteon}, and William Yeager's ``Ships in the Night''
multiple-protocol router~\cite{ships}, which was the basis for the Stanford
startup Cisco Systems.

By the mid 1990s, software could no longer keep up with the demand for higher
link speeds caused by the rapid adoption of the Internet and the World Wide
Web. Juniper Network's M40 router~\cite{m40} was an early example of a hardware
router in 1998.  The M40 contained a dedicated chip to implement the router's
data plane along with a control processor to implement the router's control
plane. As we described in Chapter~\ref{chap:intro}, since the mid 1990s, the
fastest routers have predominantly been built out of dedicated hardware because
hardware specialization is the only way to sustain the yearly increases in link
speeds (Figure~\ref{fig:router_evolution}).

\subsection{Active Networks (mid 1990s)}
The mid 1990s saw the development of active networks~\cite{ants, switchware}, an
approach that advocated that the network be programmable or ``active'' to allow
the deployment of new services in the network infrastructure. There were at
least two approaches to active networks. First, the programmable router
approach~\cite{switchware}, which allowed a network operator to program a
router in a restricted manner. Second, the capsule approach~\cite{ants,
wetherall_thesis, anet_retrospective}, where end hosts would embed programs
into packets as capsules, which would then be executed by the router.

Active networks came to be associated mostly with the capsule
approach~\cite{sdn_history}. But the capsule approach raised some security
concerns. Because programs were embedded into packets by end users, it was
possible that a malicious or erroneous end user program could corrupt the
entire router.  One way to resolve security concerns was to execute the capsule
program within an isolated application-level virtual machine like the Java
virtual machine~\cite{ants, wetherall_thesis, anet_retrospective}. However,
this came at the cost of degraded forwarding performance. As an example, the
SNAP~\cite{snap_active_packets} system that prototyped the capsule approach
reported a forwarding rate of 100 Mbit/s in 2001, about two orders of magnitude
slower than the Juniper M40 40 Gbit/s hardware router developed in
1998~\cite{m40}.
%TODO: SNAP does not use a JVM. I think it uses other isolation techniques.
%This part could be improved.

The capsule approach---probably the most ambitious of all active networking
visions---has not panned out in its most general form due to security concerns.
However, recent systems~\cite{int} have exposed a far more restricted subset of
a router's features to end hosts (\eg the ability for an end host to read
router state, but not write to it), reminiscent of the capsule approach.  On
the other hand, the programmable router approach {\em has} seen adoption in
various forms: software-defined networking and programmable router chips both
provide network operators with different kinds of restricted router
programmability.

\subsection{Software routers (1999 to present)}
One approach to programmability since the late 1990s has been to use a
general-purpose substrate for writing packet processing programs, in contrast
to fixed-function router hardware that can not be programmed. The
general-purpose substrate has varied over the years. For instance,
Click~\cite{click} used a single-core CPU in 2000.  In the early 2000s, Intel
introduced a line of processors tailored towards networking called network
processors, such as the IXP1200~\cite{ixp1200} in 2000 and the
IXP2800~\cite{ixp2800} in 2002.  The RouteBricks project used a multi-core
processor~\cite{routebricks} in 2009, the PacketShader project used a graphics
processor (GPU)~\cite{packetshader} in 2010, and the NetFPGA-SUME project used
an FPGA in 2014~\cite{netfpga}.

Software routers have found adoption as a means of programming routers at the
expense of performance. They have been especially beneficial in scenarios where
the link speeds are lower, but the computational requirements are higher.  For
instance, this approach has been used to implement MAC layer algorithms in
WiFi~\cite{samplerate, roofnet, xor} and signal processing algorithms in the
wireless physical layer~\cite{sora, cloudiq}.

Many domain-specific languages (DSLs) for packet processing were developed in
parallel with the development of software routers. For instance,
Click~\cite{click} uses C++ for packet processing on software routers.
packetC~\cite{packetc} and Microengine C~\cite{microenginec} target network
processors. When designing the Domino DSL (Chapter~\ref{chap:domino}, our
syntax and semantics were inspired by these DSLs.  However, because Domino
targets line-rate routers, it is more constrained. For instance, because
compiled programs run at line rate, \pktlanguage forbids loops.

%,TODO: cite somewhere: Intel's auto-partitioning C
%compiler~\cite{intel_uiuc_pldi}, Maybe in related compiler techniques?

\subsection{Software-defined networking (2004 to now)}
Starting in the early 2000s, researchers argued for separating the router's
control plane (\ie the part of the router that runs a distributed routing
protocol to computes its routing tables) from the router's data plane (\ie the
part of the router that actually forwards packets by looking them up in the
routing table)~\cite{route_control, case_for, forces, fourd, ethane}. As an
example, the implementation of a link-state routing protocol would be part of
the control plane, while the implementation of a longest-prefix-based table
lookup would be part of the data plane.

The idea behind this approach, which later came to be called software-defined
networking (SDN)~\cite{sdn_coining}, was that much of the flexibility desired
by network operators when managing large-scale networks (\eg traffic
engineering, access control, creating virtual networks) had to do with the
control plane, not the data plane.  Furthermore, the control plane executed
relatively infrequently compared with the data plane: once every few
milliseconds, as opposed to once every few nanoseconds. Hence, while the data
plane had to be implemented in hardware for performance, the relatively
infrequent nature of the control plane's operations allowed it to be moved out
of the router and on to a commodity general-purpose processor, where it could
be much more easily programmed.

SDN also introduced the idea of centralized control: the idea that by moving
router control planes off the router and on to a general-purpose processor, the
entire control plane for the network could be centralized at a few servers.
This, in turn, allowed those few servers to compute routes for the entire
network with the benefit of global network visibility. Effectively, SDN
replaced an error-prone distributed route computation protocol with a much
simpler centralized graph computation (\eg shortest paths using Dijkstra's
algorithm) to compute the routes.

SDN also required a mechanism for the control plane to populate the contents of
the routing tables once the control plane had computed routes for each router.
These tables would then be consulted by the data plane when forwarding packets.
The most well-known of these mechanisms was the OpenFlow API~\cite{openflow},
which exposed a minimal interface to the routing tables in router hardware.
The goal of OpenFlow was to serve as a minimum common denominator across
interfaces to routing tables in different forwarding chips. This was so that
existing chips could immediately support the OpenFlow API.

While the OpenFlow API made it possible to program the network's control plane,
it did not necessarily make it easy. Hence, the development of SDN also led to
the development of many high-level programming languages to program a router's
control plane~\cite{frenetic, pyretic}. While SDN saw a considerable amount of
research work in programming and verifying rich control plane policies, it saw
much lesser work in enabling programmability in the data plane or the router
chip itself.

\subsection{Network functions virtualization (2012 to now)}
Network functions virtualization (NFV)~\cite{nfv_etsi_2012} sought to move
richer packet processing functionality, beyond vanilla packet forwarding, onto
commodity general-purpose processors and the cloud
infrastructure~\cite{aplomb}. This packet processing functionality included
deep-packet inspection, load balancing, intrusion detection, and WAN
acceleration, colloquially unified under the umbrella term ``middlebox.''
Several systems emerged to program both the data~\cite{netbricks} and
control~\cite{opennf} plane of such middleboxes.

One common use case for such middleboxes is at the edge of a network (\eg at a
cellular base station), where a variety of packet-processing functions run on a
cluster of processors~\cite{e2} every time a client accesses the Internet.
Because NFV carries out its packet processing on a software platform, it can
also be viewed as a practical use case for software routers at the edge where
the link rate requirements are modest.
%TODO: cite domain 2.0

\subsection{Edge-based or end-host-based software-defined networking (2013 to now)}
It soon became clear that the OpenFlow API was not expressive enough to capture
all the needs of network operators because it was designed as a common minimum
denominator API for easy adoption. The lack of expressiveness in OpenFlow led
to the edge-based approach to software-defined networking~\cite{fabric_sdn,
nvp, openvswitch}.

In this approach, the network's routers were divided into two classes. The {\em
edge routers} were located at the entrance or edge of a network and performed
programmable packet manipulation. At the center of the network were the {\em
core routers} that simply forwarded packets with little to no programmability.
Because the edge routers were spatially distributed to serve clients in
different locations, each edge router had to only handle a small slice of the
overall aggregate traffic into and out of the network.

As a result, the edge routers had far less demanding performance requirements
relative to the core, which allowed them to be implemented on general-purpose
CPUs.  Using a general-purpose CPU for programming edge routers allowed them to
be far more programmable than the restricted OpenFlow API for routers would
have allowed. Open Virtual Switch~\cite{openvswitch} is one well-known example
of an edge router; it runs within the hypervisor on an end host and connects
together multiple virtual machines on a single end host to the network.  More
recently, increasing performance requirements on the edge have led to
implementing such virtual switches on an FPGA~\cite{daniel_firestone_nsdi}.

Taken to its logical extreme, the edge router could be the end host itself.
Hence, when discussing edge-based approaches, we also include several recent
proposals that use the end hosts to achieve network flexibility. For instance,
Eden~\cite{eden} provides a programmable data plane using commodity routers by
programming end hosts alone.  Tiny Packet Programs (TPP)~\cite{tpp} allow end
hosts to embed small programs in packet headers, which are then executed by the
router in a style similar to capsule-based active networks. TPPs use a
restricted instruction set to alleviate the performance and security concerns
of capsule-based active networks. On the measurement and monitoring side, many
systems monitor network performance from end hosts alone~\cite{netpoirot,
minlan-snap, dapper-sosr, trumpet, azure-smartnic}.

Edge-based or end-host-based solutions are necessary for application context
that is not available within the network. For instance, knowledge of which
application used the network, which may be useful in monitoring, is only
available at the end hosts.  Similarly, many network security applications,
such as filtering spam are best run on the end hosts because the information
required to determine what is spam and what is not is best left on the end host
for privacy reasons. Furthermore, quite a bit of programmable networking
functionality can be carried out at the edge (\eg network virtualization,
access control, security policies etc.) with no loss in correctness.
% TODO: Other examples beside spam detection

However, the edge-based approach is inadequate for all network problems. For
instance, there is a substantial improvement in performance from using network
support for congestion control (\eg DCTCP~\cite{dctcp} that uses explicit
congestion notification support from the routers and XCP~\cite{xcp} that uses
explicit information on the extent of congestion from the routers). Many other
recent proposals for improving network performance rely on support from the
routers within the core of the network~\cite{pFabric, pias, d3, rcp, detail,
conga, letflow}. There is also a considerable improvement in network visibility
from direct in-network monitoring, in contrast to ``triangulating'' the root cause of a
network problem from network measurements from different end host vantage
points.  In summary, the absence of a programmable network core leaves
significant performance on the table and complicates network debugging.

One could argue for a hybrid network architecture that combines edge-based
programmability with smarter, but fixed, core routers. Such an architecture
still puts all the programmability at the edge but augments core routers with a
small set of fixed features to support programmability from the edge.  Examples
of this approach include Universal Packet Scheduling~\cite{ups} and in-band
network telemetry~\cite{int, int_paper}, which augment routers with a
fine-grained priority queue and the ability to export queue size information
into packets, but otherwise leave all the programmability to the edge.

This hybrid approach would be future proof and general if there existed a small
set of features that could then be deemed sufficient for a fixed-function
router. If such a small set of fixed features did indeed exist, it would be
preferable and simpler to build a fixed-function router rather than a
programmable router. But the last few decades of increasing feature creep in a
router suggest that such a feature set is quite unlikely. In such a setting,
where features are constantly under churn, programmability within the network
provides future proofness and peace of mind for the network operator: the
ability to quickly add features to a router when required without lengthy
hardware iteration cycles.

\section{Concurrent research on network programmability (2013 to now)}
\label{s:concurrent}

\subsection{Programmable router chips}
\label{ss:prog_router_chips}
Besides the edge-based approach to network programmability, another response to
the difficulty of repeatedly extending OpenFlow was to develop a programmable
router chip. This router chip would have a minimal instruction set that would
permit a network operator to express OpenFlow-like forwarding rules for {\em
any} new protocol format. This is in contrast to OpenFlow, which only supports
a restricted and fixed set of actions as part of the forwarding rules (drop,
decrement TTL, forward, etc.) on a restricted and fixed set of packet headers
(UDP, TCP, IP, etc.).

Recent academic work~\cite{rmt} and commercial router chips~\cite{tofino,
flexpipe, xpliant} embody this approach of building routers that are both fast
and programmable. The P4 programming language~\cite{p4} has emerged as an
industry effort towards a standard programming language for these chips.  P4
compilers~\cite{lavanya_compiler} then compile these P4 programs to
programmable router architectures such as the RMT~\cite{rmt} and
FlexPipe~\cite{flexpipe} architectures.

To the extent that we know from publicly available documents, these chips
provide flexibility on only two counts: recognizing user-specific header
formats and programmatically manipulating packet headers for functions such as
forwarding, tunneling, and access control, \ie functions that do not modify
router state.

In particular, they do not provide the programmability required to implement
many of the grayed-out algorithms in Figure~\ref{fig:router_evolution}.  These
algorithms require the ability to programmatically manipulate router state on
every packet, the ability to program which packet a router link must schedule
next, and the ability to program what per-flow statistics a router must measure
for a large number of flows. We hope this dissertation suggests directions for
the future evolution of such programmable router chips.

\subsection{Centralized data planes}
Fastpass~\cite{fastpass} manages a datacenter network at fine time scales using
a logically centralized arbiter server that regulates packet transmissions from all
end hosts within the network. A logically centralized arbiter provides the
global visibility needed to perform resource management for the entire network.
Put differently, Fastpass adopts the idea of centralized control from SDN, but
applies it to the data plane instead of the control plane.

In Fastpass, when an application on an end host calls send() on a socket, the
end host sends a message to the arbiter specifying its network demands. Upon
receiving this message, the arbiter specifies the times at which the end host
must transmit and the network path that the end host's packets must follow
during those times. 

In subsequent work, Flexplane~\cite{flexplane} uses the arbiter as an emulator
to emulate existing resource management algorithms (\eg active queue management
algorithms like RED to manage the router's buffer and scheduling algorithms
like WFQ to schedule the router's links).  Flowtune~\cite{flowtune} applies
Fastpass-style ideas to the centralized management of groups of packets called
flowlets, instead of packets.  This allows the centralized arbiter in Flowtune
to handle larger networks than Fastpass.

Fastpass, Flexplane, and Flowtune are all applicable to networks of a modest
size, until the centralized arbiter becomes a bottleneck.  Microbenchmarks that
stress the Flowtune arbiter alone show that an arbiter running on a 64-core
server can manage a network with an aggregate capacity of 184
Tbit/s~\cite{flowtune}. However, the largest network that a Fastpass-style
approach has been benchmarked on (in the Flexplane paper~\cite{flexplane}) is a
network of 40 10-Gbit/s servers connected to a single top-of-rack router.

In summary, by leveraging a centralized data plane implemented on a commodity
CPU, Fastpass provides far more flexibility than a restricted interface exposed
by a programmable router chip---but at a significant scalability cost.

\subsection{Stateful packet-processing}
SNAP~\cite{snap} programs stateful data-plane algorithms using a network
transaction: an atomic block of code that treats the entire network as one
router~\cite{onebigswitch}. It then uses a compiler to translate network
transactions into rules on each router. SNAP needs a lower level compiler to
compile these router-local rules to a router's pipeline and can use
\pktlanguage for this purpose. In fact, we compile several examples from SNAP
to study the generality of our atoms in Table~\ref{tab:atoms_generalize}.
FAST~\cite{fast} provides router support and software abstractions for state
machines. As \S\ref{s:eval} shows, atoms support more general stateful
processing beyond state machines that enable a much wider class of data-plane
algorithms.

\subsection{Universal Packet Scheduling (UPS)} UPS~\cite{ups} shares our goal
of flexible packet scheduling. UPS approaches this goal by seeking a single
scheduling algorithm that is {\em universal} and can emulate any scheduling
algorithm. Theoretically, UPS finds that the well-known LSTF scheduling
discipline~\cite{lstf} is universal if packet departure times for the
scheduling algorithm to be emulated are known up front. Practically, UPS shows
that by appropriately initializing slacks, many different scheduling objectives
can be emulated using LSTF. LSTF is programmable using PIFOs, but the set of
schemes practically expressible using LSTF is limited. For example, LSTF cannot
express:
\begin{CompactEnumerate}
\item Hierarchical scheduling algorithms such as HPFQ, because it uses only one
priority queue.
\item Non-work-conserving algorithms. For such algorithms LSTF must know the
departure time of each packet up-front, which is not practical.
\item Short-term bandwidth fairness in fair queueing, because LSTF maintains no
  router state except one priority queue. As shown earlier in
  Figure~\ref{fig:wfq_trans}, programming a fair queueing algorithm requires us
  to maintain a virtual time state variable. Without this, a new flow could have
  arbitrary virtual start times, and be deprived of its fair share indefinitely.
  UPS provides a fix to this that requires estimating fair shares periodically,
  which is hard to do in practice in an environment where flows continuously start
  and stop.
  %TODO: Need to fix this short-term bandwidth fairness point above. Look at LSTF's
  % implementation from the NSDI paper and ensure what I say here is consistent.
  % Right now the point above talks about a hypothetical virtual time based impl.
  % of fair queueing using LSTF. But LSTF doesn't use virtual time.
\end{CompactEnumerate}
The restrictions in UPS/LSTF are a result of a limited programming model. UPS
assumes that routers are fixed and cannot be programmed to modify packet fields
or manipulate router state. Further, it only has a single priority queue.  By
using atom pipelines to execute scheduling and shaping transactions, and by
composing multiple PIFOs together, PIFOs express a wider class of scheduling
algorithms.

\subsection{Measurements using sketches.} Several systems for network
measurement use summary data structures that give up some accuracy to fit
measurement information within limited router memory~\cite{univmon, flowradar,
counterbraids, dream, progme, opensketch}. These systems provide traffic volume
statistics (\ie statistics derived from flow counts) using summary data
structures like sketches that tradeoff accuracy for low memory consumption.
Since counting is linear-in-state and can be scalably implemented in
\TheSystem, \TheSystem sidesteps this tradeoff. Instead, \TheSystem trades off
memory size with cache eviction rate (\Sec{eval}).  \TheSystem also allows
users to perform a broader set of aggregations (the linear-in-state class)
beyond counters without losing accuracy.

\subsection{Languages for network measurement} Prior network query
languages~\cite{gigascope, frenetic, path_query, streaming-monitoring} allow
users to ask questions primarily about traffic volumes, since their input data
is collected using NetFlow and match-action rule counters~\cite{openflow}. In
contrast, \TheSystem enables expressive {\em performance} questions on data
collected with purpose-built router hardware. \TheSystem shares some language
constructs with Gigascope~\cite{gigascope} and
Sonata~\cite{streaming-monitoring}, but supports aggregations directly in the
router.

\subsection{Router-based monitoring.} Traditionally, router-based monitoring
has focused on per-flow counts, not performance measurement. For example,
NetFlow~\cite{netflow} and sFlow~\cite{sflow} provide traffic summaries through
flow and packet sampling. Packet-capture systems~\cite{cisco-span, niksun,
netsight, everflow, pathdump, path_query} collect entire packets or digests.
These approaches sample extensively to lower collection overheads and are
useful for posthoc traffic analysis. However, they do not capture details of
{\em performance} phenomena (\eg TCP incast) as specified by a flexible
language like \TheSystem.

\subsection{Recent router support for measurement} In-band Network Telemetry
(INT)~\cite{int, int_paper} exposes queue lengths to end hosts by stamping it
on the packet itself. \TheSystem builds on INT and provides flexible filters
and aggregations {\em directly in routers}.  \TheSystem's data aggregation in
routers saves the bandwidth needed to collect INT data distributed over many
end hosts. Besides, with INT alone, performance information may be lost, since
packets carrying the INT data may be dropped on the way to end hosts. The
Tetration chip provides flow-level telemetry, exposing a fixed set of metrics
including latency, window and packet size variation, and a ``burst
measurement''~\cite{tetration-telemetry}. In contrast, \TheSystem provides
programmable aggregation functions and flow definitions, \eg input/output port
versus 5-tuple.

\section{Summary}
This chapter summarized both the historic background that sets up the context
for this dissertation and the related work that is concurrent with this
dissertation. The main difference in approach between this dissertation and
prior approaches to router programmability is the joint design of both
hardware and software.

Jointly designing hardware and software provides us more leverage than treating
either as unchangeable. In the process, it also allows us to obtain both
programmability and performance for some classes of router functionality.  Past
approaches either propose hardware or software improvements, while treating the
other as fixed. We briefly describe 5 examples of how this additional leverage
helps the systems in this dissertations relative to systems in prior work.

\Para{Universal Packet Scheduling.} UPS adds a fine-grained priority queue to a
{\em fixed} router, but otherwise treats the router hardware as unmodifiable,
opting to carry out priority assignments in software at the edge. Relative to
UPS, PIFOs are able to express a broader set of scheduling algorithms because
we develop new programmable router hardware (atoms) to maintain and update
state on the router.

\Para{Flexplane.} Flexplane moves the data plane into slower commodity hardware
to emulate algorithms like RED and WFQ. By modifying router hardware to support
atoms,  Domino allows such AQM schemes to be programmed directly without
emulation on high-speed router hardware.

\Para{Hardware support for counters.} Many router hardware designs optimize for
the fixed software requirement of per-flow counters~\cite{counter_shah}.
\TheSystem develops an expressive programming language for measurements in
software along with hardware support for this language. This allows \TheSystem
to support a wider range of queries beyond counters.

\Para{CONGA.} CONGA~\cite{conga} is a load-balancing algorithm that uses
congestion-aware flowlet-based load balancing to respond better to asymmetry in
the network topology. CONGA is implemented on a custom chip by developing new
router hardware for the sole purpose of load balancing. In contrast, we
developed a programming model in software as part of Domino in addition to the
atom hardware. This allows us to reuse the same programmable atoms for many
different algorithms including CONGA.

\Para{SONATA.} SONATA~\cite{streaming-monitoring} is a measurement system that
sources its measurements from statistics exported by OpenFlow. By designing new
router hardware for programmable measurement, \TheSystem can express a wider
range of measurements than what is supported by OpenFlow.
