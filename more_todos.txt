1. Move related work ahead as background.
2. More detailed intro.
3. Figures in intro.
4. Move new SIGCOMM text into performance queries.
5. More Domino programs.
6. Move text of theorems into main text.
7. Figure out how to keep marple's tech report and dissertation in sync.
8. Reduce text from each chapter's introduction.
9. http://www.srl.inf.ethz.ch/papers/sosp15-symple.pdf is very related work to our linear-in-state characterization:

This system (called Symple) shares many of the same goals as Marple's aggregation
implementation: http://www.srl.inf.ethz.ch/papers/sosp15-symple.pdf
The introduction should be sufficient to understand the problem they
are tackling.

Figure 1 captures an example of an aggregation that they can
parallelize (we should see if Figure 1 is linear-in-state), while
Figure 2 captures the essence of their approach. Some of their
restrictions in section 4.3 seem similar to ours. For instance,
1. They encodes integers as affine transformations of other integers.
2. They only allows multiplication of integers only with constants, and
3. They don't permit division operations on integers.

It might be worthwhile understanding more deeply what the differences
and similarities between Marple and Symple are.

10a. Add a multiply-accumulate atom to the Domino set. The only reason we
didn't add it to the original Domino paper was because queue size information
for RED was not available at enqueue. We knew of it all the way back to the
2013 HotNets paper.

10. Add a subsection on the generalizability of the Domino atoms.
i.e., take the Domino atoms as of some time,
and talk about whether all the algorithms that we wrote afterward could map to those atoms/not

For instance, all the Domino code snippets, maybe STFQ in the PIFO paper,
several PIFO examples, and the shift register that Domino uses. It's fair to
say that the linear-in-state atom is also an example of generalizability
because we should have added RED and hence EWMA to the instruction set early
on. Could also add PIE etc.

OK, we can't add STFQ because we had to modify the atom a little bit to support
STFQ
(https://github.com/packet-transactions/domino-examples/commit/344a8f74bbfaf37d63c479354cec38a4db24e08a)
But I think DNS ttl change detection was an example of something where we didn't
have to modify the atom, but we'll have to rerurn it on the old atom to be sure.
Could add HULA to the list of Domino algorithms
Could also try implementing HashPipe and some more algorithms from SNAP,
and the papers that SNAP itself references.

11. talk about why things don't generalize beyond the glib reason that CoDel needs floating point.
Maybe separate the discussion into the structure of the atom and the details of the atom, i.e., all atom updates can be written out as if(guard) then atom. The detail is in how complicated a stateful guard is. This is a useful pattern to extract out of the atom design process. The structure is probably going to be future proof even if the stateful predicates themselves aren't

12. Spend a decent amount of time explaining the machine model: packets come in, they are looked up, we carry out some action, and so on so forth.

13. Distinguish single-pipeline and multi-pipeline routers and clarify that we
only deal with the former here.

14. The term fixed-function is a bit misleading.

15. Proof of fixed-point algorithm in Marple

16. Cite these papers (http://www.cc.gatech.edu/projects/canes/papers/icnp97.pdf and
http://web.mit.edu/Saltzer/www/publications/endtoend/ANe2ecomment.html)
because they relate the end-to-end argument with the active networks work. Some
of it might be applicable to programmable routers as well. I guess one differnce
relative to either capsule-based or switchware-based active networks is that we
assume the programmability is something the network operator wants, not an application
driven thing. So we are only providing useful programmability. Further, there isn't as much
application heterogeneity in this private context (WANs, DCs) as opposed to the Internet.

17. Fragment from David Wetherall's dissertation that is telling:

"To design applications in this manner, a useful view is that of network
processing as an optimization of end-to-end processing. This is compatible with
reliability concerns and partial deployment. For example, in an online auction
service, Legedza delegated bid rejection processing - but not bid acceptance
processing - to network nodes [Wetherall et al., 1998]. Bid rejection within
the network is an optimization because if it does not occur within the network
then it can be handled at the edge of the network. On the other hand, bid
acceptance cannot be modeled as an optimization because if it occurs within the
network it must be reliably signaled to the edge of the network."

I think the differences relative to active networks are that:
(1) The applications/users program active networks as opposed to network operators here.
(2) Hence the programs need to be isolated (e.g., JVM) degrading performance.
(3) Also, you need to compose services/programs from multiple different operators, which
hurts predictability (ref. http://web.mit.edu/Saltzer/www/publications/endtoend/ANe2ecomment.html) and also makes it hard to design the capsule execution system.

Here, we have the network operator programming the network. The programming model is
quite restricted and there are typically just a few applications using these networks
(e.g., private WANs, DCs, etc.) Correspondingly, Clark's concerns from the original 1988
paper about in-network functionality (e.g., reliability) affecting other applications are
not as applicable anymore.

