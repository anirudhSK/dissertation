\section{Introduction}
\label{sec:intro}
%Effective monitoring can
%also help evaluate new techniques for improving network performance, \eg
%\cite{hedera, dctcp, pfabric}.

Effective {\em performance monitoring} of large networks is crucial to quickly
localize problems, such as high queueing latencies~\cite{int}, TCP
incast~\cite{tcpincast}, %% microbursts~\cite{tpp}, 
and load imbalance~\cite{conga}.

A common approach to network monitoring is to collect information from the
endpoint network stack~\cite{minlan-snap, dapper-sosr, pathdump, trumpet} or
end-to-end probes~\cite{pingmesh} to diagnose performance problems.  While
endpoints provide application context, they lack visibility to localize
performance problems in links deep in the network. For example, it is
challenging to localize queue buildup to a particular switch, or pinpoint
traffic causing the queue buildup, forcing operators to infer the network-level
root causes indirectly~\cite{pingmesh}.
%% Endpoints only see the end-to-end symptom of a
%% problem, not its root cause (\eg it is challenging to attribute high end-to-end
%% latency to queue buildup on a particular switch queue). Some
%% systems\cite{pingmesh, netsight} infer root causes by combining endpoint
%% measurements, but combining endpoint measurements requires network bandwidth to
%% stream endpoint information to collection servers.

Switch-based monitoring could allow operators to diagnose problems with more
direct visibility into performance statistics. However, traditional switch
mechanisms like sampling~\cite{netflow, sflow}, mirroring~\cite{cisco-span,
  netsight, everflow} and counting~\cite{cormode, univmon} are limited in enabling
performance monitoring.
%
Sampling and mirroring miss events of interest as it is infeasible to collect
information on all packets, while counters only track traffic volume
statistics. None of these mechanisms provides relevant performance data, like
queueing delays.
%% Switches provide traffic information through packet sampling (\eg
%% NetFlow~\cite{netflow}, sflow~\cite{sflow}), packet mirroring (\eg
%% SPAN~\cite{cisco-span}), and counting~\cite{cormode}. These mechanisms do not
%% expose performance information like queue depths, making performance diagnoses
%% challenging.

%endpoint vs. end host
Some upcoming technologies recognize the need for better performance monitoring
using switches. In-band network telemetry~\cite{int} writes queueing delays
experienced by a packet on the packet itself, allowing endpoints to localize
delay spikes. The Tetration switching chip~\cite{tetration-telemetry} provides a
`flow cache' that measures flow-level performance metrics.
%
These metrics are useful, but they are exposed at a fixed granularity, and
aggregated with fixed functions. For example, the list of exposed metrics
includes flow-level latency and packet size variation, but not latency
variation, \ie jitter.
%% While these efforts expose useful performance information, they only compute a
%% fixed set of flow-level performance metrics.  Given the ever-changing needs of
%% operators, fixed hardware invariably falls short of an operator's needs.
%% Because hardware redesign is expensive,  it is worthwhile building switch
%% measurement {\em primitives} that support programmable performance diagnoses as
%% opposed to measurement {\em functionality} that supports a useful---but
%% fixed--set of diagnoses. While we share this goal with programmable
%% switches~\cite{xpliant, tofino, flexpipe, corsa}, today's programmable switch
%% architectures only support programmable parsing~\cite{gibb_parsing}, header
%% processing~\cite{rmt, domino} and scheduling~\cite{pifo}.
% not fine-grained programmable measurement. 

Operator requirements are ever-changing, and redesigning hardware is
expensive. We believe that the trajectory of adding fixed-function switch
monitoring piecemeal is unsustainable. Instead, we should build performance
monitoring {\em primitives} that can be flexibly reused for a variety of
operator needs.
%% Given the
%% ever-changing needs of operators, fixed hardware invariably falls short of an
%% operator's needs.  Because hardware redesign is expensive, it is worthwhile
%% building switch measurement {\em primitives} that support programmable
%% performance diagnoses as opposed to measurement {\em functionality} that support
%% a useful---but fixed--set of diagnoses. 
%% While we share this goal with programmable switches~\cite{xpliant, tofino,
%%   flexpipe, corsa}, today's programmable switch architectures only support
%% programmable parsing~\cite{gibb_parsing}, header processing~\cite{rmt, domino}
%% and scheduling~\cite{pifo}.
% not fine-grained programmable measurement.
Today's programmable switches support flexible parsing~\cite{gibb_parsing},
header processing~\cite{rmt, domino_sigcomm}, and
scheduling~\cite{pifo_sigcomm}. Our goal is to add monitoring to this list.

%TODO: Not sure if this language-directed is over-the-top? It seems to
%complicate things.
% Anirudh: I might just remove this and retain language-directed in the title
% alone.
In this paper, we propose {\em language-directed hardware design}: we design a
language that can express a broad variety of performance monitoring use cases,
and then design high-speed switch hardware primitives in service of this
language.
%% By designing hardware to support an expressive language, 
We believe the resulting hardware design can support a wide variety of current
and future performance monitoring needs.

% Anirudh: again, we say stream here, but log in the next section :)
\Para{Performance query language.} We propose {\em \TheSystem}, a language that
uses familiar functional primitives like {\ct map, filter} and {\ct fold} for
performance monitoring.  \TheSystem provides the abstraction of a stream that
contains performance information for {\em every packet} at {\em every queue} in
the network (\Sec{language}). Using \TheSystem, programmers can focus their
attention on traffic experiencing interesting performance (\eg get packets with
high queueing latencies), aggregate information across packets in flexible ways
(\eg compute a moving average over queueing latency per flow), and detect
simultaneous performance conditions in the network (\eg detect when the queue
depth is large {\em and} the number of connections using the queue is high).
\TheSystem also enables composing multiple queries to compute more complex
statistics, \eg a flowlet~\cite{flowlets} size histogram to evaluate
load balancing~\cite{conga}.
%TODO: Do we need to talk about multiple queries?
%TODO: what is the \cite{flare} paper for flowlet sizes?

\Para{Hardware design for performance queries.} To support the \TheSystem query
language, we are able to leverage many existing switch hardware primitives like
INT~\cite{int} and flexible header processing~\cite{rmt}. To support flexible
aggregations over packets, we design a {\em programmable key-value store} in
hardware (\Sec{aggregation}), where the keys represent flow identifiers and values
represent the state computed by the aggregation function. This key-value store
should support two requirements: it should update values at a line-rate of 1
packet per clock cycle (at 1GHz~\cite{rmt, xpliant_sdk}), and support millions
of keys (\ie connections). Unfortunately, neither SRAM nor DRAM memory is fast
and dense enough to meet both requirements.
%% Second, we design a programmable
%% key-value store in hardware (\Sec{hardware}) to support flexible aggregations
%% over packets. The keys represent flow identifiers and the values represent the
%% state computed by the aggregation function, such as a counter. This key-value
%% store should be fast enough for line-rate operation and large enough to hold
%% millions of flows. To support both requirements, 

We split the key-value store into a small but fast on-chip {\em cache} in SRAM,
and a larger but slower off-chip {\em backing store} in DRAM. Traditional caches
incur variable write latencies due to cache misses; however, line-rate packet
forwarding requires deterministic latency bounds. Our design treats a cache miss
as the arrival of a packet from a new flow, and {\em merges} the new cache value
with the the flow's old value in the backing store on eviction. The backing
store can reside either on the switch CPU or a separate collection server.
% TODO: backend or backing store? Need to be consistent.

We characterize a class of affine aggregation functions which we call {\em
  linear-in-state} for which
such merging is possible without losing accuracy. Many aggregation functions are
linear-in-state, \eg counters, predicated counters (\eg count only TCP packets
that saw timeouts), exponentially weighted moving averages, or functions
computed over a finite window of packets.  We design a switch instruction to
support linear-in-state functions, finding that it easily meets
timing at 1 GHz, while occupying modest silicon area.

\Para{Query compiler.} We implement a {\em compiler} that takes \TheSystem
queries and compiles them into switch configurations for two targets
(\Sec{compiler}): (1) the P4 behavioral model~\cite{p4-bmv2}, an open source
programmable software switch, and (2) a simulator for high-speed switch
hardware, Banzai~\cite{domino_sigcomm}, that can be used to experiment with
different instruction sets.  The \TheSystem compiler detects linear-in-state
aggregations in input queries and successfully targets the linear-in-state switch
instruction that we added to Banzai.

% TODO: Need to clean up paragraph above.
% Rewrite paragraph once eval is in to stress four points: Domino, Mininet, Traces, REDIS
We show that the \TheSystem language can express a wide variety of useful
performance monitoring examples, like detecting and localizing TCP incast, and
measuring the prevalence of out-of-order TCP packets. \TheSystem queries require
between 5-10 pipeline stages for almost all queries, which is modest for a
32-stage pipeline~\cite{rmt}. We evaluate our key-value store's performance
using trace-driven simulations. For a 32-Mbit on-chip cache, which occupies
about 2.5\% of the chip's area, the typical cache eviction rate from a single
top-of-rack switch can be handled by a single 8-core server running
REDIS.
